{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inspect columns\n",
    "  - info / describe (5 number summary)\n",
    "  - nunique / unique\n",
    "- Histograms `df.hist(figsize=(18, 10))` -> skewness / kurtosis / outliers\n",
    "- Box plots\n",
    "- Scatterplot / `sns.pairplot(data)` `pd.plotting.scatter_matrix(data_pred,figsize  = [9, 9])`\n",
    "- Data types `df.isna()` / `df.isna().sum()`\n",
    "- Nulls, drop columns\n",
    "- Multicollinearity\n",
    "- Sub dataframe\n",
    "- Scaling and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Data scrubbing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Binning\n",
    "\n",
    "`df[\"binned_markdown_\"] = pd.cut(df.Column, 5, labels=['10%', '20%'])`\n",
    "\n",
    "- Replacing Nulls\n",
    "\n",
    "`df.Column.replace(np.NaN, \"NaN\", inplace=True)`\n",
    "\n",
    "- Dropping columns\n",
    "\n",
    "`to_drop = ['col1', 'col2']\n",
    "df.drop(to_drop, axis=1, inplace=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`sns.set(style=\"white\")\n",
    "corr = df.corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .5})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- `df.Column = (df.Column - df.Column.mean()) / df.Column.std()`\n",
    "- `walmart_log[\"Weekly_Sales\"]= np.log(walmart_log[\"Weekly_Sales\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T21:59:39.779005Z",
     "start_time": "2019-06-10T21:59:39.774007Z"
    },
    "hidden": true
   },
   "source": [
    "`df.Column = (df.Column - df.Column.mean()) / df.Column.std()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Binning\n",
    "- Label encoding\n",
    "- Dummy variables / one-hot encoding `one_hot_df = pd.get_dummies(df)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- KDE\n",
    "\n",
    "`for column in ['Col1','Col2']:\n",
    "    df[column].plot.hist(normed = True)\n",
    "    df[column].plot.kde(label = column)\n",
    "    plt.legend()\n",
    "    plt.show()`\n",
    "    \n",
    "- Join Plot\n",
    "\n",
    "`for for column in ['Col1','Col2']:\n",
    "    sns.jointplot(x=column, y=\"TargetCol\",\n",
    "                  data=df, \n",
    "                  kind='reg', \n",
    "                  label=column,\n",
    "                  joint_kws={'line_kws':{'color':'green'}})\n",
    "    plt.legend()\n",
    "    plt.show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model steps:\n",
    "- scatter plot\n",
    "- distributions of dependent and independent variables\n",
    "\n",
    "Test:\n",
    "- Linearity (scatter plots). Check for outliers\n",
    "- Normality: **model residuals** should follow a normal distribution (histograms or Q-Q plots)\n",
    "- Homoscedasticity <> Heteroscedasticity: dependent variable variability (scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plt.scatter(df.height, df.weight)\n",
    "df.plot.kde()`\n",
    "\n",
    "`df[column].plot.hist(normed=True, label = column + ' histogram')\n",
    "df[column].plot.kde(label = column + ' kde')`\n",
    "\n",
    "- Linearity\n",
    "\n",
    "`fig, axs = plt.subplots(1, 3, sharey=True, figsize=(18, 6))\n",
    "for idx, channel in enumerate(['TV', 'radio', 'newspaper']):\n",
    "    df.plot(kind='scatter', x=channel, y='sales', ax=axs[idx], label=channel)\n",
    "plt.legend()\n",
    "plt.show()`\n",
    "\n",
    "- OLS (Ordinary Least Square regression)\n",
    "\n",
    "`import statsmodels.formula.api as smf`\n",
    "\n",
    "`f = 'weight~height'\n",
    "model = ols(formula=f, data=df).fit()\n",
    "model.summary()`\n",
    "\n",
    "`for idx, val in enumerate(col_names):\n",
    "    print (\"Walmart: Weekly_Sales~\" + val)\n",
    "    print (\"------------------------------\")\n",
    "    f = 'Weekly_Sales~' + val\n",
    "    model = smf.ols(formula=f, data=walmart).fit()\n",
    "    results.append([val, model.rsquared, model.params[0], model.params[1], model.pvalues[1] ])\n",
    "    print(results[idx+1])`\n",
    "    \n",
    "`walmart_final = walmart_log.drop([\"Fuel_Price\",\"Unemployment\",\"IsHoliday\", \"Store_9\", \"Dept_99\",\"Type_B\"], axis=1)\n",
    "walmart_final.columns[92:121]\n",
    "walmart_final =walmart_final.drop(walmart_final.columns[92:121],axis=1)\n",
    "walmart_final.describe()`\n",
    "\n",
    "- Drop continuous variables which resulted in single linear models with a R-squared value <0.01 for the walmart_log models.\n",
    "- Drop 1 column for each categorical variable we end up using.\n",
    "\n",
    "**Note Intercept**: association vs. causation\n",
    "\n",
    "Prediction:\n",
    "\n",
    "`new_df = pd.DataFrame({'TV': [df.TV.min(), df.TV.max()]})\n",
    "model.predict(new_df)`\n",
    "\n",
    "Error terms:\n",
    "\n",
    "`fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"height\", fig=fig)\n",
    "plt.show()`\n",
    "\n",
    "Q-Q Plots:\n",
    "\n",
    "`residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "fig.show()`\n",
    "\n",
    "Jarque-Bera test:\n",
    "\n",
    "JB value of roughly 6 or higher indicates that errors are not normally distributed. Close to 0: normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identify multicollinearity\n",
    "  - Scatter matrix `pd.plotting.scatter_matrix(data_pred,figsize  = [9, 9])`\n",
    "  - Correlation matrix `data_pred.corr()` `abs(data_pred.corr()) > 0.75`\n",
    "  - Seaborn heatmap `sns.heatmap(data_pred.corr(), center=0)`\n",
    "- Remove problematic features `df = df.drop('col', axis=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coef\n",
    "\"How does Y change for each additional unit X' (where X' is the (log- and min-max, standardized,...))\"\n",
    "\n",
    "### $R^2$\n",
    "- R-squared uses a baseline model which is the worst model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and mean y values.\n",
    "\n",
    "$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}= \\dfrac{\\sum_i y_i-\\hat{y_i}}{\\sum_i{Y_i-\\bar{y_i}}}$$\n",
    "\n",
    "-  The problem with $R^2$ is that, whichever predictor you **add** to your model which will make your model more complex, will increase your $R^2$ value. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read [this blogpost](https://www.statisticshowto.datasciencecentral.com/adjusted-r2/) on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n",
    "\n",
    "### P-value\n",
    "Just like with single linear regression, the parameters or coefficients we're calculating have a p-value or *significance* attached to them. The interpretation of the p-value for each parameter is exactly the same as for single multiple regression: \n",
    "\n",
    "> The p-value represents the probability that the coefficient is actually zero.\n",
    "\n",
    "In the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn't significant.\n",
    "\n",
    "The two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
